# -*- coding: utf-8 -*-
"""Last_Attempt

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yf0ogq_SFnB6zpcKv32-3l5kGduSWfQH
"""

# --- Cell 1: Installations ---
# This cell installs the necessary system-level OCR engine and all required Python libraries.

# Install Tesseract OCR
!apt-get install -y tesseract-ocr

# Install Python libraries for AI, PDF processing, and the RAG pipeline
!pip install -q langchain langchain-community sentence-transformers faiss-cpu
!pip install -q transformers pymupdf pillow bitsandbytes accelerate pytesseract

# --- Cell 2: Imports ---
# All the necessary libraries for the entire notebook are imported here.

# Core Python and File Management
import os
import fitz  # PyMuPDF
from PIL import Image
import torch
import pytesseract

# LangChain Components
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

# Hugging Face Transformers for Vision and Language Models
# FIX: Added BitsAndBytesConfig to handle quantization explicitly
from transformers import AutoProcessor, LlavaForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# Google Colab specific
from google.colab import files

# Suppress warnings for a cleaner output
import warnings
warnings.filterwarnings("ignore")

# Create a folder for uploaded documents
upload_folder = "/content/uploaded_docs"
os.makedirs(upload_folder, exist_ok=True)

# Use Colab's file uploader
print("Please upload your PDF files.")
uploaded = files.upload()

# Move uploaded files to the dedicated folder
for filename in uploaded.keys():
    os.rename(filename, os.path.join(upload_folder, filename))

print(f"\n✅ Uploaded files moved to {upload_folder}:", os.listdir(upload_folder))

print("Loading Vision Model (LLaVA)...")
model_id = "llava-hf/llava-1.5-7b-hf"

# Create an explicit quantization configuration to solve potential runtime errors.
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

vision_processor = AutoProcessor.from_pretrained(model_id)
vision_model = LlavaForConditionalGeneration.from_pretrained(
    model_id,
    quantization_config=quantization_config, # Use the explicit config
    device_map="auto"
)

print("✅ Vision model loaded successfully.")

def extract_pdf_content(pdf_path, image_output_dir):
    """Extracts text from each page and saves images from a PDF."""
    os.makedirs(image_output_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    text_content_by_page = []
    image_paths_by_page = {}

    for page_num, page in enumerate(doc):
        text_content_by_page.append(page.get_text())
        image_list = page.get_images(full=True)
        for img_index, img in enumerate(image_list):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]

            image_filename = f"page_{page_num+1}_img_{img_index}.{image_ext}"
            image_path = os.path.join(image_output_dir, image_filename)
            with open(image_path, "wb") as f:
                f.write(image_bytes)

            if page_num not in image_paths_by_page:
                image_paths_by_page[page_num] = []
            image_paths_by_page[page_num].append(image_path)

    return text_content_by_page, image_paths_by_page


def generate_caption(image_path, model, processor):
    """Generates a text caption for a given image using the LLaVA model."""
    raw_image = Image.open(image_path).convert('RGB')
    prompt = "USER: <image>\nDescribe this image in detail. It is a map, figure, or diagram from a history document. Explain what it is about. ASSISTANT:"

    inputs = processor(text=prompt, images=raw_image, return_tensors="pt").to(model.device)

    output = model.generate(**inputs, max_new_tokens=200)
    caption = processor.decode(output[0], skip_special_tokens=True)

    assistant_keyword = "ASSISTANT:"
    caption = caption[caption.find(assistant_keyword) + len(assistant_keyword):].strip()
    return caption

def extract_text_from_image(image_path):
    """Extracts text from an image using Tesseract OCR."""
    try:
        text = pytesseract.image_to_string(Image.open(image_path))
        return text
    except Exception as e:
        return f"Error during OCR: {e}"

print("✅ Helper functions (LLaVA captioning and Tesseract OCR) are defined.")

upload_folder = "/content/uploaded_docs"
image_folder = "/content/extracted_images"
all_rich_docs = []

os.makedirs(upload_folder, exist_ok=True)

for filename in os.listdir(upload_folder):
    if filename.lower().endswith(".pdf"):
        pdf_path = os.path.join(upload_folder, filename)
        print(f"Processing {pdf_path}...")

        texts, images_by_page = extract_pdf_content(pdf_path, image_folder)

        captions_by_page = {}
        for page_num, image_paths in images_by_page.items():
            page_captions = []
            print(f"  - Analyzing {len(image_paths)} images on page {page_num + 1}...")
            for img_path in image_paths:
                try:
                    llava_caption = generate_caption(img_path, vision_model, vision_processor)
                    ocr_text = extract_text_from_image(img_path)

                    combined_description = (
                        f"VISUAL DESCRIPTION: {llava_caption}\n\n"
                        f"TEXT EXTRACTED FROM IMAGE (OCR):\n---\n{ocr_text}\n---"
                    )
                    page_captions.append(combined_description)

                except Exception as e:
                    print(f"    - Error processing image {img_path}: {e}")
            captions_by_page[page_num] = page_captions

        for page_num, page_text in enumerate(texts):
            page_content = page_text
            if page_num in captions_by_page:
                rich_image_descriptions = "\n\n".join(captions_by_page[page_num])
                page_content += f"\n\n--- Image Content Analysis ---\n{rich_image_descriptions}"

            doc = Document(page_content=page_content, metadata={"source": filename, "page": page_num + 1})
            all_rich_docs.append(doc)

print(f"\n✅ Created {len(all_rich_docs)} documents with combined text, LLaVA descriptions, and OCR text.")

print("Splitting documents into chunks...")
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
chunks = splitter.split_documents(all_rich_docs)
print(f"✅ Total chunks created: {len(chunks)}")

print("Creating vector store...")
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.from_documents(chunks, embedding_model)
print("✅ Vector store ready!")

print("Loading Text Generation Model (Qwen)...")
text_model_id = "Qwen/Qwen1.5-1.8B-Chat"
tokenizer = AutoTokenizer.from_pretrained(text_model_id)
llm = AutoModelForCausalLM.from_pretrained(text_model_id, device_map="auto")
print("✅ Qwen 1.8B model loaded")

# --- Replace your Cell 10 with this corrected version ---

def generate_answer(question, context):
    """Generates an answer based on the provided context and question."""
    context = context[:2000]

    messages = [
        {
            "role": "system",
            "content": (
                "You are a factual assistant. Your ONLY source of information is the text provided in the 'Context' section. "
                "You are to answer the user's 'Question' based *only* on the information in the 'Context'. "
                "Do NOT use any external knowledge. Do not invent any details, dates, names, or numbers that are not explicitly in the 'Context'. "
                "If the answer is not available in the context, you MUST reply with the exact phrase: 'Not available in the document.'"
            )
        },
        {
            "role": "user",
            "content": (
                f"Context:\n{context}\n\n"
                f"Question: {question}\n\n"
                "Based on the context provided, answer the question comprehensively. If the question asks for a list, provide the full list."
            )
        }
    ]

    text_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text_prompt, return_tensors="pt").to(llm.device)

    # --- FIX APPLIED HERE ---
    # Unpack the 'inputs' dictionary into keyword arguments using **
    outputs = llm.generate(**inputs, max_new_tokens=256, do_sample=False, temperature=0.0)

    # Decode only the newly generated part of the output
    answer = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)

    return answer.strip()


def rag_pipeline(question, k=10):
    """The main RAG pipeline: retrieves context and generates an answer."""
    retrieved_docs = db.similarity_search(question, k=k)
    unique_contexts = list(dict.fromkeys([doc.page_content.strip() for doc in retrieved_docs]))
    context = "\n\n".join(unique_contexts)
    return generate_answer(question, context)

print("✅ RAG pipeline functions are defined and corrected.")

print("\n--- Starting the RAG system. Type 'exit' to end. ---")
while True:
    question = input("\nEnter your question: ")
    if question.lower().strip() == "exit":
        print("✅ Exiting RAG system.")
        break
    answer = rag_pipeline(question)
    print("\n✅ Answer:", answer)